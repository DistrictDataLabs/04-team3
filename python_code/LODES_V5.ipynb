{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LODES Data Analysis\n",
    "## Prepare Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from urllib2 import urlopen\n",
    "from StringIO import StringIO\n",
    "import gzip\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\dcapizzi\\\\Documents\\\\GitHub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-fd30d69afb92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set create working folder and set as active directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\Users\\dcapizzi\\Documents\\GitHub'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\dcapizzi\\\\Documents\\\\GitHub'"
     ]
    }
   ],
   "source": [
    "# Set create working folder and set as active directory\n",
    "os.chdir('C:\\Users\\dcapizzi\\Documents\\GitHub')\n",
    "if not os.path.exists('lodes'):\n",
    "    os.makedirs('lodes')\n",
    "    \n",
    "os.chdir('C:\\Users\\dcapizzi\\Documents\\GitHub\\lodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load initial LODES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect user input for the year and states to download for the analysis\n",
    "year = raw_input('Enter a year: ')\n",
    "input_list = raw_input(\"Enter states to include  separated by commas (no spaces): \")\n",
    "state_list = input_list.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define final data frames to aggregate all state data\n",
    "\n",
    "lodes_columns = ['w_geocode', 'h_geocode', 'tot_jobs', 'age_29_bel_jobs',\n",
    "       'age_30_54_jobs', 'age_55_over_jobs', 'sal_1250_bel_jobs',\n",
    "       'sal_1250_3333_jobs', 'sal_3333_over_jobs', 'goods_prod_jobs',\n",
    "       'trade_transp_jobs', 'all_other_svc_jobs', 'createdate', 'state',\n",
    "       'w_block', 'h_block', 'w_2010_block', 'w_state', 'w_county_name',\n",
    "       'w_block_group_code', 'w_block_group_name', 'w_metro_name',\n",
    "       'w_zip_code', 'w_place_name', 'w_county_sub_name', 'w_createdate',\n",
    "       'h_2010_block', 'h_state', 'h_county_name', 'h_block_group_code',\n",
    "       'h_block_group_name', 'h_metro_name', 'h_zip_code', 'h_place_name',\n",
    "       'h_county_sub_name', 'h_createdate']\n",
    "\n",
    "lodes_data = pd.DataFrame([],columns=lodes_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionaries to house downloaded files\n",
    "\n",
    "dict_lodes = {}\n",
    "dict_xwalk = {}\n",
    "\n",
    "# Loop through all states selected by user, download the relevant files from the Census website, unzip, read, and load into dictionaries \n",
    "# Process takes some time, please be patient\n",
    "\n",
    "for state in state_list:\n",
    "    \n",
    "    # Sets url for primary \"LODES\" data set - which provides data on the home Census block, work Census block, and commuters in between\n",
    "    lodes_url = 'http://lehd.ces.census.gov/data/lodes/LODES7/' + state.lower() + '/od/' + state.lower() + '_od_main_JT00_' + year + '.csv.gz'\n",
    "    \n",
    "    # Sets url for \"cross-walk\" data with the city, state, ZIP, etc. for each Census block\n",
    "    xwalk_url = 'http://lehd.ces.census.gov/data/lodes/LODES7/' + state.lower() + '/' + state.lower() + '_xwalk.csv.gz'\n",
    "    \n",
    "    # Names the files\n",
    "    lodes_filename = 'lodes_' + state + \"_\" + year + '.csv.gz'\n",
    "    xwalk_filename =  'xwalk_' + state + \"_\" + year + '.csv.gz'\n",
    "    \n",
    "    # Downloads the files\n",
    "    urllib.urlretrieve(lodes_url, lodes_filename)\n",
    "    urllib.urlretrieve(xwalk_url, xwalk_filename)\n",
    "    \n",
    "    print 'Data downloaded for '+state\n",
    "    \n",
    "    # Unzips the files\n",
    "    unzip_lodes = gzip.open(lodes_filename, 'rb')\n",
    "    unzip_xwalk = gzip.open(xwalk_filename, 'rb')\n",
    "    \n",
    "    # Reads the files to disk \n",
    "    unzip_lodes = unzip_lodes.read()\n",
    "    unzip_xwalk = unzip_xwalk.read()\n",
    "\n",
    "    # Saves as objects in teh created dictionaries \n",
    "    dict_lodes[state]=pd.read_csv(StringIO(unzip_lodes))\n",
    "    dict_xwalk[state]=pd.read_csv(StringIO(unzip_xwalk))\n",
    "    print 'Data tables created for '+state\n",
    "    \n",
    "    # Removes unnecessary fields and names the columns to consistent, human-readable names\n",
    "    dict_lodes[state].columns = ['w_geocode','h_geocode','tot_jobs','age_29_bel_jobs',\n",
    "              'age_30_54_jobs','age_55_over_jobs','sal_1250_bel_jobs','sal_1250_3333_jobs','sal_3333_over_jobs',\n",
    "              'goods_prod_jobs','trade_transp_jobs','all_other_svc_jobs','createdate']\n",
    "\n",
    "    dict_xwalk[state] = DataFrame(dict_xwalk[state],columns=['tabblk2010','stusps','ctyname', 'bgrp','bgrpname','cbsaname','zcta','stplcname','ctycsubname','createdate'])\n",
    "    dict_xwalk[state].columns = ['2010_block', 'state', 'county_name', 'block_group_code', 'block_group_name','metro_name', 'zip_code','place_name', 'county_sub_name','createdate']\n",
    "    \n",
    "    print 'Column names defined for '+state\n",
    "    \n",
    "    # Creates 'block-group-level' field to join LODES to xwalk and centroid lat/longs (Census block group codes are the first 12 digits of Census block codes)\n",
    "    left = lambda x: str(int(x))[:12]\n",
    "    dict_lodes[state]['w_block'] = dict_lodes[state]['w_geocode'].apply(left)\n",
    "    dict_lodes[state]['w_block'] = dict_lodes[state]['w_geocode'].apply(left)\n",
    "    dict_lodes[state]['h_block'] = dict_lodes[state]['h_geocode'].apply(left)\n",
    "    dict_xwalk[state]['block_group_code']= dict_xwalk[state]['block_group_code'].apply(left)\n",
    "    \n",
    "    dict_lodes[state]['state'] = state\n",
    "    \n",
    "    print 'New fields created for '+state\n",
    "    \n",
    "print 'Process complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create blank dictionaries to join or merge cross-walk data with LODES data\n",
    "\n",
    "dict_xwalk_w = {}\n",
    "dict_xwalk_h = {}\n",
    "\n",
    "# Duplicay (copy) cross-walk data, with columns one for work, one for home\n",
    "for state in dict_xwalk:\n",
    "    dict_xwalk_w[state] = deepcopy(dict_xwalk[state]) \n",
    "    dict_xwalk_h[state] = deepcopy(dict_xwalk[state]) \n",
    "    dict_xwalk_w[state].rename(columns=lambda x: \"w_\"+x, inplace=\"True\")\n",
    "    dict_xwalk_h[state].rename(columns=lambda x: \"h_\"+x, inplace=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each state in dict_lodes, merge once on the \"work\" Census block (w_geocode) and once on the \"home\" Census block (h_geocode)\n",
    "# This data will provide an idea of the city/state/zip for both the work and home block code groups\n",
    "\n",
    "for state in dict_lodes:\n",
    "    dict_lodes[state] = pd.merge(dict_lodes[state], dict_xwalk_w[state], how='left', left_on='w_geocode', right_on='w_2010_block')\n",
    "    dict_lodes[state] = pd.merge(dict_lodes[state], dict_xwalk_h[state], how='left', left_on='h_geocode', right_on='h_2010_block')\n",
    "    lodes_data = lodes_data.append(dict_lodes[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lodes_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform LODES data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new field \"home to work\" with both home and work geocodes\n",
    "lodes_data['unique_id'] = lodes_data['h_geocode'].map('{0:f}'.format).astype(str).apply(lambda x: x[:15]) + ' to ' + lodes_data['w_geocode'].map('{0:f}'.format).astype(str).apply(lambda x: x[:15]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take new data set, and split into \"home\" and \"work\" tables to be flattened\n",
    "\n",
    "lodes_data_home = DataFrame(lodes_data, columns = ['unique_id','h_geocode', 'tot_jobs', 'age_29_bel_jobs',\n",
    "       'age_30_54_jobs', 'age_55_over_jobs', 'sal_1250_bel_jobs',\n",
    "       'sal_1250_3333_jobs', 'sal_3333_over_jobs', 'goods_prod_jobs',\n",
    "       'trade_transp_jobs', 'all_other_svc_jobs',\n",
    "       'h_block', 'h_state', 'h_county_name',\n",
    "       'h_block_group_code', 'h_block_group_name', 'h_metro_name',\n",
    "       'h_zip_code', 'h_place_name', 'h_county_sub_name'])\n",
    "lodes_data_home['type']='Home'\n",
    "lodes_data_home['path']=1\n",
    "\n",
    "lodes_data_work = DataFrame(lodes_data, columns = ['unique_id','w_geocode', 'tot_jobs', 'age_29_bel_jobs',\n",
    "       'age_30_54_jobs', 'age_55_over_jobs', 'sal_1250_bel_jobs',\n",
    "       'sal_1250_3333_jobs', 'sal_3333_over_jobs', 'goods_prod_jobs',\n",
    "       'trade_transp_jobs', 'all_other_svc_jobs',\n",
    "       'w_block', 'w_state', 'w_county_name',\n",
    "       'w_block_group_code', 'w_block_group_name', 'w_metro_name',\n",
    "       'w_zip_code', 'w_place_name', 'w_county_sub_name'])\n",
    "\n",
    "lodes_data_work['type']='Work'\n",
    "lodes_data_work['path']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rename columns to be the same for both new tables\n",
    "new_columns = ['unique_id','geocode', 'tot_jobs', 'age_29_bel_jobs',\n",
    "       'age_30_54_jobs', 'age_55_over_jobs', 'sal_1250_bel_jobs',\n",
    "       'sal_1250_3333_jobs', 'sal_3333_over_jobs', 'goods_prod_jobs',\n",
    "       'trade_transp_jobs', 'all_other_svc_jobs',\n",
    "       'block', 'state', 'county_name',\n",
    "       'block_group_code', 'block_group_name', 'metro_name',\n",
    "       'zip_code', 'place_name', 'county_sub_name','type','path']\n",
    "\n",
    "lodes_data_home.columns = new_columns\n",
    "lodes_data_work.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append both tables and sort by Path ID\n",
    "lodes_data_flat = lodes_data_home.append(lodes_data_work)\n",
    "lodes_data_flat = lodes_data_flat.sort(['unique_id','path']).reset_index(drop=True)\n",
    "lodes_data_flat[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional data on latitude, longitude, and demographics into data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in data with latitudes, longitudes, and other data sources\n",
    "latlong = pd.read_csv('DDL_census_data.csv')\n",
    "\n",
    "# Rename columns\n",
    "latlong.columns = ['state', 'county', 'tract', 'blockgrouppiece', 'full_geo_id', 'geoid',\n",
    "       'name', u'lsad', 'land_area', 'water_area', 'latitude', 'longitude', 'id',\n",
    "       'geoid2', 'geoid3', 'geo_display','median_income','moe_median_income',\n",
    "       'geoid4', 'geoid5', 'geo_display2', 'total','moe_total:',\n",
    "       'foodstamps','moe_foodstamps',\n",
    "       'foodstamps_disability','moe_foodstamps_disability','foodstamps_nodisability','moe_foodstamps_nodisability',\n",
    "       'nofoodstamps','moe_nofoodstamps',\n",
    "       'nofoodstamps_disability','moe_nofoodstamps_disability',\n",
    "       'nofoodstamps_nodisability','moe_nofoodstamps_nodisability']\n",
    "\n",
    "# Reformat columns\n",
    "latlong['full_geo_id'] = latlong['full_geo_id'].apply(lambda x: x[9:])\n",
    "\n",
    "# Eliminate unnecessary columns\n",
    "latlong = DataFrame(latlong, columns = ['full_geo_id', 'latitude', 'longitude',\n",
    "        'foodstamps','moe_foodstamps',\n",
    "       'foodstamps_disability','moe_foodstamps_disability','foodstamps_nodisability','moe_foodstamps_nodisability',\n",
    "       'nofoodstamps','moe_nofoodstamps',\n",
    "       'nofoodstamps_disability','moe_nofoodstamps_disability',\n",
    "       'nofoodstamps_nodisability','moe_nofoodstamps_nodisability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lodes_data_full = pd.merge(lodes_data_flat, latlong, how='left', left_on='block_group_code', right_on='full_geo_id') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional data on transit for metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lodes_data_full['category']='lodes'\n",
    "lodes_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "sqlite_file = 'sqlite://///Users/Kruthika/Projects/DDL/04-team3/census_v2.db'\n",
    "engine = create_engine(sqlite_file)\n",
    "from pandas.io import sql\n",
    "sql.execute('DROP TABLE IF EXISTS lodes_data',engine)\n",
    "lodes_data_full.to_sql('lodes_data', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get station-level descriptive data from WMATA API, including latitude and longitude of stations and line codes\n",
    "r = requests.get('https://api.wmata.com/Rail.svc/json/jStations?api_key=fb7119a0d3464673825a26e94db74451')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for entrances in r.json()['Stations']:\n",
    "    for e in entrances.keys():\n",
    "        if e not in data_list:\n",
    "            data_list.append(e)\n",
    "print data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metro_stations = json_normalize(r.json()['Stations'])\n",
    "metro_stations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metro_stations.to_csv('stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get bus route descriptive data from WMATA API, including latitude and longitude of stations and route codes\n",
    "r1 = requests.get('https://api.wmata.com/Bus.svc/json/jStops?api_key=fb7119a0d3464673825a26e94db74451')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops_list = []\n",
    "for stops in r1.json()['Stops']:\n",
    "    for s in stops.keys():\n",
    "        if s not in stops_list:\n",
    "            stops_list.append(s)\n",
    "print stops_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_stops = json_normalize(r1.json()['Stops'])\n",
    "bus_stops.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = bus_stops.apply(lambda x: pd.Series(x['Routes']),axis=1).stack().reset_index(level=1, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.name = 'Routes'\n",
    "bus_routes = bus_stops.drop('Routes', axis=1).join(s)\n",
    "bus_routes['category'] = 'bus'\n",
    "bus_routes['type'] = 'bus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_routes.columns = ['latitude','longitude','name','unique','detail','category','type']\n",
    "bus_routes[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bus_routes.to_csv('busroutes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get path-level train  data from WMATA API, including latitude and longitude of stations and line codes\n",
    "rblue = requests.get('https://api.wmata.com/Rail.svc/json/jPath?FromStationCode=J03&ToStationCode=G05&api_key=fb7119a0d3464673825a26e94db74451')\n",
    "rgreen = requests.get('https://api.wmata.com/Rail.svc/json/jPath?FromStationCode=F11&ToStationCode=E10&api_key=fb7119a0d3464673825a26e94db74451')\n",
    "rorange = requests.get('https://api.wmata.com/Rail.svc/json/jPath?FromStationCode=K08&ToStationCode=D13&api_key=fb7119a0d3464673825a26e94db74451')\n",
    "rred = requests.get('https://api.wmata.com/Rail.svc/json/jPath?FromStationCode=A15&ToStationCode=B11&api_key=fb7119a0d3464673825a26e94db74451')\n",
    "rsilver = requests.get('https://api.wmata.com/Rail.svc/json/jPath?FromStationCode=N06&ToStationCode=G05&api_key=fb7119a0d3464673825a26e94db74451')\n",
    "ryellow = requests.get('https://api.wmata.com/Rail.svc/json/jPath?FromStationCode=C15&ToStationCode=E06&api_key=fb7119a0d3464673825a26e94db74451')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for paths in rblue.json()['Path']:\n",
    "    for p in paths.keys():\n",
    "        if p not in data_list:\n",
    "            data_list.append(p)\n",
    "print data_list\n",
    "\n",
    "dfblue = json_normalize(rblue.json()['Path'])\n",
    "dfgreen = json_normalize(rgreen.json()['Path'])\n",
    "dforange = json_normalize(rorange.json()['Path'])\n",
    "dfred = json_normalize(rred.json()['Path'])\n",
    "dfsilver = json_normalize(rsilver.json()['Path'])\n",
    "dfyellow = json_normalize(ryellow.json()['Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metro_lines = pd.concat([dfblue, dfgreen, dforange, dfred, dfsilver, dfyellow], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metro_lines.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metro_combined = pd.merge(metro_lines, metro_stations, how='left', left_on='StationCode', right_on='Code')\n",
    "metro_combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metro_combined = DataFrame(metro_combined,columns=['LineCode','SeqNum', 'StationName','Address.City','Address.State','Address.Zip','Lat','Lon'])\n",
    "metro_combined.columns = ['unique','path','name','metro_name','state','zip','latitude','longitude']\n",
    "metro_combined['type']='train'\n",
    "metro_combined['category']='train'\n",
    "metro_combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metro_combined.to_csv('trainandroute.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blend all data sets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lodes_transit_data = pd.concat([lodes_data_full, bus_routes, metro_combined], ignore_index=True)\n",
    "lodes_transit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "sqlite_file = 'sqlite://///Users/Kruthika/Projects/DDL/04-team3/census.db'\n",
    "engine = create_engine(sqlite_file)\n",
    "from pandas.io import sql\n",
    "sql.execute('DROP TABLE IF EXISTS lodes_data',engine)\n",
    "lodes_transit_data.to_sql('lodes_transit_data', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lodes_transit_data [lodes_transit_data['category']=='train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(lodes_transit_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lodes_transit_data.to_csv('lodes_final_output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
